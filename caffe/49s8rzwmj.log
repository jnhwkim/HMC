I0112 22:21:59.577067 1994072832 caffe.cpp:99] Use GPU with device ID 0
I0112 22:21:59.888948 1994072832 caffe.cpp:107] Starting Optimization
I0112 22:21:59.888978 1994072832 solver.cpp:32] Initializing solver from parameters: 
test_iter: 800
test_interval: 100000
base_lr: 0.01
display: 10000
max_iter: 10000000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 100000
snapshot_prefix: ".49s8rzwmj"
solver_mode: GPU
random_seed: 1710
net_param {
  name: "HMC"
  layers {
    top: "data"
    top: "label"
    name: "mnist"
    type: DATA
    data_param {
      source: "49s8rzmj_tr_ldb"
      batch_size: 480
      backend: LEVELDB
    }
    include {
      phase: TRAIN
    }
    transform_param {
      scale: 0.00390625
    }
  }
  layers {
    top: "data"
    top: "label"
    name: "mnist"
    type: DATA
    data_param {
      source: "49s8rzmj_te_ldb"
      batch_size: 100
      backend: LEVELDB
    }
    include {
      phase: TEST
    }
    transform_param {
      scale: 0.00390625
    }
  }
  layers {
    bottom: "data"
    top: "ip1"
    name: "ip1"
    type: INNER_PRODUCT
    blobs_lr: 1
    blobs_lr: 2
    inner_product_param {
      num_output: 800
      weight_filler {
        type: "xavier"
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layers {
    bottom: "ip1"
    top: "ip1"
    name: "relu1"
    type: RELU
  }
  layers {
    bottom: "ip1"
    top: "ip2"
    name: "ip2"
    type: INNER_PRODUCT
    blobs_lr: 1
    blobs_lr: 2
    inner_product_param {
      num_output: 500
      weight_filler {
        type: "xavier"
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layers {
    bottom: "ip2"
    top: "ip2"
    name: "relu2"
    type: RELU
  }
  layers {
    bottom: "ip2"
    top: "ip3"
    name: "ip3"
    type: INNER_PRODUCT
    blobs_lr: 1
    blobs_lr: 2
    inner_product_param {
      num_output: 10
      weight_filler {
        type: "xavier"
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layers {
    bottom: "ip3"
    bottom: "label"
    top: "accuracy"
    name: "accuracy"
    type: ACCURACY
  }
  layers {
    bottom: "ip3"
    bottom: "label"
    top: "loss"
    name: "loss"
    type: SOFTMAX_LOSS
  }
}
test_state {
  stage: "test-on-test-set"
}
I0112 22:21:59.889591 1994072832 solver.cpp:63] Creating training net specified in net_param.
I0112 22:21:59.890027 1994072832 net.cpp:275] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0112 22:21:59.890370 1994072832 net.cpp:39] Initializing net from parameters: 
name: "HMC"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "49s8rzmj_tr_ldb"
    batch_size: 480
    backend: LEVELDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 800
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "ip2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "ip2"
  top: "ip3"
  name: "ip3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  name: "accuracy"
  type: ACCURACY
}
layers {
  bottom: "ip3"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I0112 22:21:59.890646 1994072832 net.cpp:67] Creating Layer mnist
I0112 22:21:59.890655 1994072832 net.cpp:356] mnist -> data
I0112 22:21:59.890671 1994072832 net.cpp:356] mnist -> label
I0112 22:21:59.890679 1994072832 net.cpp:96] Setting up mnist
I0112 22:21:59.890729 1994072832 data_layer.cpp:45] Opening leveldb 49s8rzmj_tr_ldb
I0112 22:21:59.922106 1994072832 data_layer.cpp:128] output data size: 480,1,1,245
I0112 22:21:59.922232 1994072832 net.cpp:103] Top shape: 480 1 1 245 (117600)
I0112 22:21:59.922250 1994072832 net.cpp:103] Top shape: 480 1 1 1 (480)
I0112 22:21:59.922261 1994072832 net.cpp:67] Creating Layer label_mnist_1_split
I0112 22:21:59.922266 1994072832 net.cpp:394] label_mnist_1_split <- label
I0112 22:21:59.922278 1994072832 net.cpp:356] label_mnist_1_split -> label_mnist_1_split_0
I0112 22:21:59.922286 1994072832 net.cpp:356] label_mnist_1_split -> label_mnist_1_split_1
I0112 22:21:59.922291 1994072832 net.cpp:96] Setting up label_mnist_1_split
I0112 22:21:59.922296 1994072832 net.cpp:103] Top shape: 480 1 1 1 (480)
I0112 22:21:59.922298 1994072832 net.cpp:103] Top shape: 480 1 1 1 (480)
I0112 22:21:59.922304 1994072832 net.cpp:67] Creating Layer ip1
I0112 22:21:59.922307 1994072832 net.cpp:394] ip1 <- data
I0112 22:21:59.922312 1994072832 net.cpp:356] ip1 -> ip1
I0112 22:21:59.922317 1994072832 net.cpp:96] Setting up ip1
I0112 22:21:59.924088 1994072832 net.cpp:103] Top shape: 480 800 1 1 (384000)
I0112 22:21:59.924115 1994072832 net.cpp:67] Creating Layer relu1
I0112 22:21:59.924120 1994072832 net.cpp:394] relu1 <- ip1
I0112 22:21:59.924126 1994072832 net.cpp:345] relu1 -> ip1 (in-place)
I0112 22:21:59.924132 1994072832 net.cpp:96] Setting up relu1
I0112 22:21:59.924136 1994072832 net.cpp:103] Top shape: 480 800 1 1 (384000)
I0112 22:21:59.924147 1994072832 net.cpp:67] Creating Layer ip2
I0112 22:21:59.924150 1994072832 net.cpp:394] ip2 <- ip1
I0112 22:21:59.924154 1994072832 net.cpp:356] ip2 -> ip2
I0112 22:21:59.924160 1994072832 net.cpp:96] Setting up ip2
I0112 22:21:59.927112 1994072832 net.cpp:103] Top shape: 480 500 1 1 (240000)
I0112 22:21:59.927157 1994072832 net.cpp:67] Creating Layer relu2
I0112 22:21:59.927161 1994072832 net.cpp:394] relu2 <- ip2
I0112 22:21:59.927167 1994072832 net.cpp:345] relu2 -> ip2 (in-place)
I0112 22:21:59.927173 1994072832 net.cpp:96] Setting up relu2
I0112 22:21:59.927176 1994072832 net.cpp:103] Top shape: 480 500 1 1 (240000)
I0112 22:21:59.927182 1994072832 net.cpp:67] Creating Layer ip3
I0112 22:21:59.927187 1994072832 net.cpp:394] ip3 <- ip2
I0112 22:21:59.927191 1994072832 net.cpp:356] ip3 -> ip3
I0112 22:21:59.927197 1994072832 net.cpp:96] Setting up ip3
I0112 22:21:59.927253 1994072832 net.cpp:103] Top shape: 480 10 1 1 (4800)
I0112 22:21:59.927274 1994072832 net.cpp:67] Creating Layer ip3_ip3_0_split
I0112 22:21:59.927287 1994072832 net.cpp:394] ip3_ip3_0_split <- ip3
I0112 22:21:59.927304 1994072832 net.cpp:356] ip3_ip3_0_split -> ip3_ip3_0_split_0
I0112 22:21:59.927345 1994072832 net.cpp:356] ip3_ip3_0_split -> ip3_ip3_0_split_1
I0112 22:21:59.927353 1994072832 net.cpp:96] Setting up ip3_ip3_0_split
I0112 22:21:59.927358 1994072832 net.cpp:103] Top shape: 480 10 1 1 (4800)
I0112 22:21:59.927362 1994072832 net.cpp:103] Top shape: 480 10 1 1 (4800)
I0112 22:21:59.927376 1994072832 net.cpp:67] Creating Layer accuracy
I0112 22:21:59.927379 1994072832 net.cpp:394] accuracy <- ip3_ip3_0_split_0
I0112 22:21:59.927383 1994072832 net.cpp:394] accuracy <- label_mnist_1_split_0
I0112 22:21:59.927387 1994072832 net.cpp:356] accuracy -> accuracy
I0112 22:21:59.927392 1994072832 net.cpp:96] Setting up accuracy
I0112 22:21:59.927405 1994072832 net.cpp:103] Top shape: 1 1 1 1 (1)
I0112 22:21:59.927463 1994072832 net.cpp:67] Creating Layer loss
I0112 22:21:59.927469 1994072832 net.cpp:394] loss <- ip3_ip3_0_split_1
I0112 22:21:59.927475 1994072832 net.cpp:394] loss <- label_mnist_1_split_1
I0112 22:21:59.927479 1994072832 net.cpp:356] loss -> loss
I0112 22:21:59.927485 1994072832 net.cpp:96] Setting up loss
I0112 22:21:59.927500 1994072832 net.cpp:103] Top shape: 1 1 1 1 (1)
I0112 22:21:59.927502 1994072832 net.cpp:109]     with loss weight 1
I0112 22:21:59.927516 1994072832 net.cpp:170] loss needs backward computation.
I0112 22:21:59.927520 1994072832 net.cpp:172] accuracy does not need backward computation.
I0112 22:21:59.927551 1994072832 net.cpp:170] ip3_ip3_0_split needs backward computation.
I0112 22:21:59.927553 1994072832 net.cpp:170] ip3 needs backward computation.
I0112 22:21:59.927556 1994072832 net.cpp:170] relu2 needs backward computation.
I0112 22:21:59.927559 1994072832 net.cpp:170] ip2 needs backward computation.
I0112 22:21:59.927562 1994072832 net.cpp:170] relu1 needs backward computation.
I0112 22:21:59.927566 1994072832 net.cpp:170] ip1 needs backward computation.
I0112 22:21:59.927568 1994072832 net.cpp:172] label_mnist_1_split does not need backward computation.
I0112 22:21:59.927572 1994072832 net.cpp:172] mnist does not need backward computation.
I0112 22:21:59.927574 1994072832 net.cpp:208] This network produces output accuracy
I0112 22:21:59.927579 1994072832 net.cpp:208] This network produces output loss
I0112 22:21:59.927587 1994072832 net.cpp:467] Collecting Learning Rate and Weight Decay.
I0112 22:21:59.927592 1994072832 net.cpp:219] Network initialization done.
I0112 22:21:59.927595 1994072832 net.cpp:220] Memory required for data: 5525768
I0112 22:21:59.927660 1994072832 solver.cpp:151] Creating test net (#0) specified by net_param
I0112 22:21:59.927678 1994072832 net.cpp:275] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0112 22:21:59.927687 1994072832 net.cpp:39] Initializing net from parameters: 
name: "HMC"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "49s8rzmj_te_ldb"
    batch_size: 100
    backend: LEVELDB
  }
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 800
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "ip2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "ip2"
  top: "ip3"
  name: "ip3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  name: "accuracy"
  type: ACCURACY
}
layers {
  bottom: "ip3"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TEST
  stage: "test-on-test-set"
}
I0112 22:21:59.927927 1994072832 net.cpp:67] Creating Layer mnist
I0112 22:21:59.927935 1994072832 net.cpp:356] mnist -> data
I0112 22:21:59.927943 1994072832 net.cpp:356] mnist -> label
I0112 22:21:59.927948 1994072832 net.cpp:96] Setting up mnist
I0112 22:21:59.927953 1994072832 data_layer.cpp:45] Opening leveldb 49s8rzmj_te_ldb
I0112 22:21:59.989835 1994072832 data_layer.cpp:128] output data size: 100,1,1,245
I0112 22:21:59.989881 1994072832 net.cpp:103] Top shape: 100 1 1 245 (24500)
I0112 22:21:59.989888 1994072832 net.cpp:103] Top shape: 100 1 1 1 (100)
I0112 22:21:59.989898 1994072832 net.cpp:67] Creating Layer label_mnist_1_split
I0112 22:21:59.989902 1994072832 net.cpp:394] label_mnist_1_split <- label
I0112 22:21:59.989908 1994072832 net.cpp:356] label_mnist_1_split -> label_mnist_1_split_0
I0112 22:21:59.989917 1994072832 net.cpp:356] label_mnist_1_split -> label_mnist_1_split_1
I0112 22:21:59.989923 1994072832 net.cpp:96] Setting up label_mnist_1_split
I0112 22:21:59.989927 1994072832 net.cpp:103] Top shape: 100 1 1 1 (100)
I0112 22:21:59.989930 1994072832 net.cpp:103] Top shape: 100 1 1 1 (100)
I0112 22:21:59.989936 1994072832 net.cpp:67] Creating Layer ip1
I0112 22:21:59.989939 1994072832 net.cpp:394] ip1 <- data
I0112 22:21:59.989944 1994072832 net.cpp:356] ip1 -> ip1
I0112 22:21:59.989970 1994072832 net.cpp:96] Setting up ip1
I0112 22:21:59.991536 1994072832 net.cpp:103] Top shape: 100 800 1 1 (80000)
I0112 22:21:59.991561 1994072832 net.cpp:67] Creating Layer relu1
I0112 22:21:59.991564 1994072832 net.cpp:394] relu1 <- ip1
I0112 22:21:59.991569 1994072832 net.cpp:345] relu1 -> ip1 (in-place)
I0112 22:21:59.991574 1994072832 net.cpp:96] Setting up relu1
I0112 22:21:59.991576 1994072832 net.cpp:103] Top shape: 100 800 1 1 (80000)
I0112 22:21:59.991581 1994072832 net.cpp:67] Creating Layer ip2
I0112 22:21:59.991585 1994072832 net.cpp:394] ip2 <- ip1
I0112 22:21:59.991597 1994072832 net.cpp:356] ip2 -> ip2
I0112 22:21:59.991602 1994072832 net.cpp:96] Setting up ip2
I0112 22:21:59.994586 1994072832 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0112 22:21:59.994616 1994072832 net.cpp:67] Creating Layer relu2
I0112 22:21:59.994621 1994072832 net.cpp:394] relu2 <- ip2
I0112 22:21:59.994626 1994072832 net.cpp:345] relu2 -> ip2 (in-place)
I0112 22:21:59.994632 1994072832 net.cpp:96] Setting up relu2
I0112 22:21:59.994634 1994072832 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0112 22:21:59.994649 1994072832 net.cpp:67] Creating Layer ip3
I0112 22:21:59.994653 1994072832 net.cpp:394] ip3 <- ip2
I0112 22:21:59.994663 1994072832 net.cpp:356] ip3 -> ip3
I0112 22:21:59.994669 1994072832 net.cpp:96] Setting up ip3
I0112 22:21:59.994743 1994072832 net.cpp:103] Top shape: 100 10 1 1 (1000)
I0112 22:21:59.994760 1994072832 net.cpp:67] Creating Layer ip3_ip3_0_split
I0112 22:21:59.994762 1994072832 net.cpp:394] ip3_ip3_0_split <- ip3
I0112 22:21:59.994766 1994072832 net.cpp:356] ip3_ip3_0_split -> ip3_ip3_0_split_0
I0112 22:21:59.994781 1994072832 net.cpp:356] ip3_ip3_0_split -> ip3_ip3_0_split_1
I0112 22:21:59.994788 1994072832 net.cpp:96] Setting up ip3_ip3_0_split
I0112 22:21:59.994791 1994072832 net.cpp:103] Top shape: 100 10 1 1 (1000)
I0112 22:21:59.994804 1994072832 net.cpp:103] Top shape: 100 10 1 1 (1000)
I0112 22:21:59.994809 1994072832 net.cpp:67] Creating Layer accuracy
I0112 22:21:59.994822 1994072832 net.cpp:394] accuracy <- ip3_ip3_0_split_0
I0112 22:21:59.994827 1994072832 net.cpp:394] accuracy <- label_mnist_1_split_0
I0112 22:21:59.994832 1994072832 net.cpp:356] accuracy -> accuracy
I0112 22:21:59.994837 1994072832 net.cpp:96] Setting up accuracy
I0112 22:21:59.994842 1994072832 net.cpp:103] Top shape: 1 1 1 1 (1)
I0112 22:21:59.994846 1994072832 net.cpp:67] Creating Layer loss
I0112 22:21:59.994849 1994072832 net.cpp:394] loss <- ip3_ip3_0_split_1
I0112 22:21:59.994853 1994072832 net.cpp:394] loss <- label_mnist_1_split_1
I0112 22:21:59.994858 1994072832 net.cpp:356] loss -> loss
I0112 22:21:59.994863 1994072832 net.cpp:96] Setting up loss
I0112 22:21:59.994876 1994072832 net.cpp:103] Top shape: 1 1 1 1 (1)
I0112 22:21:59.994879 1994072832 net.cpp:109]     with loss weight 1
I0112 22:21:59.994921 1994072832 net.cpp:170] loss needs backward computation.
I0112 22:21:59.994928 1994072832 net.cpp:172] accuracy does not need backward computation.
I0112 22:21:59.994932 1994072832 net.cpp:170] ip3_ip3_0_split needs backward computation.
I0112 22:21:59.994935 1994072832 net.cpp:170] ip3 needs backward computation.
I0112 22:21:59.994938 1994072832 net.cpp:170] relu2 needs backward computation.
I0112 22:21:59.994941 1994072832 net.cpp:170] ip2 needs backward computation.
I0112 22:21:59.994945 1994072832 net.cpp:170] relu1 needs backward computation.
I0112 22:21:59.994947 1994072832 net.cpp:170] ip1 needs backward computation.
I0112 22:21:59.994951 1994072832 net.cpp:172] label_mnist_1_split does not need backward computation.
I0112 22:21:59.994953 1994072832 net.cpp:172] mnist does not need backward computation.
I0112 22:21:59.994956 1994072832 net.cpp:208] This network produces output accuracy
I0112 22:21:59.994959 1994072832 net.cpp:208] This network produces output loss
I0112 22:21:59.994977 1994072832 net.cpp:467] Collecting Learning Rate and Weight Decay.
I0112 22:21:59.994990 1994072832 net.cpp:219] Network initialization done.
I0112 22:21:59.994993 1994072832 net.cpp:220] Memory required for data: 1151208
I0112 22:21:59.995107 1994072832 solver.cpp:41] Solver scaffolding done.
I0112 22:21:59.995113 1994072832 solver.cpp:160] Solving HMC
I0112 22:21:59.995151 1994072832 solver.cpp:247] Iteration 0, Testing net (#0)
I0112 22:22:00.664464 1994072832 solver.cpp:298]     Test net output #0: accuracy = 0.0974006
I0112 22:22:00.664497 1994072832 solver.cpp:298]     Test net output #1: loss = 2.30259 (* 1 = 2.30259 loss)
I0112 22:22:00.673085 1994072832 solver.cpp:191] Iteration 0, loss = 2.3026
I0112 22:22:00.673127 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.0833333
I0112 22:22:00.673144 1994072832 solver.cpp:206]     Train net output #1: loss = 2.3026 (* 1 = 2.3026 loss)
I0112 22:22:00.673167 1994072832 solver.cpp:403] Iteration 0, lr = 0.01
I0112 22:22:36.275154 1994072832 solver.cpp:191] Iteration 10000, loss = 1.35164
I0112 22:22:36.275193 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.541667
I0112 22:22:36.275202 1994072832 solver.cpp:206]     Train net output #1: loss = 1.35164 (* 1 = 1.35164 loss)
I0112 22:22:36.275255 1994072832 solver.cpp:403] Iteration 10000, lr = 0.00594604
I0112 22:23:12.269680 1994072832 solver.cpp:191] Iteration 20000, loss = 1.08758
I0112 22:23:12.269717 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.645833
I0112 22:23:12.269726 1994072832 solver.cpp:206]     Train net output #1: loss = 1.08758 (* 1 = 1.08758 loss)
I0112 22:23:12.269739 1994072832 solver.cpp:403] Iteration 20000, lr = 0.00438691
I0112 22:23:48.329869 1994072832 solver.cpp:191] Iteration 30000, loss = 1.00764
I0112 22:23:48.329906 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.689583
I0112 22:23:48.329915 1994072832 solver.cpp:206]     Train net output #1: loss = 1.00764 (* 1 = 1.00764 loss)
I0112 22:23:48.329927 1994072832 solver.cpp:403] Iteration 30000, lr = 0.00353553
I0112 22:24:23.845991 1994072832 solver.cpp:191] Iteration 40000, loss = 0.93483
I0112 22:24:23.846029 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.702083
I0112 22:24:23.846038 1994072832 solver.cpp:206]     Train net output #1: loss = 0.93483 (* 1 = 0.93483 loss)
I0112 22:24:23.846073 1994072832 solver.cpp:403] Iteration 40000, lr = 0.0029907
I0112 22:24:59.685271 1994072832 solver.cpp:191] Iteration 50000, loss = 0.863963
I0112 22:24:59.685310 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.733333
I0112 22:24:59.685319 1994072832 solver.cpp:206]     Train net output #1: loss = 0.863963 (* 1 = 0.863963 loss)
I0112 22:24:59.685355 1994072832 solver.cpp:403] Iteration 50000, lr = 0.00260847
I0112 22:25:35.150239 1994072832 solver.cpp:191] Iteration 60000, loss = 0.828115
I0112 22:25:35.150277 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.747917
I0112 22:25:35.150286 1994072832 solver.cpp:206]     Train net output #1: loss = 0.828115 (* 1 = 0.828115 loss)
I0112 22:25:35.150322 1994072832 solver.cpp:403] Iteration 60000, lr = 0.00232368
I0112 22:26:10.998497 1994072832 solver.cpp:191] Iteration 70000, loss = 0.800839
I0112 22:26:10.998536 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.758333
I0112 22:26:10.998544 1994072832 solver.cpp:206]     Train net output #1: loss = 0.800839 (* 1 = 0.800839 loss)
I0112 22:26:10.998549 1994072832 solver.cpp:403] Iteration 70000, lr = 0.00210224
I0112 22:26:46.359695 1994072832 solver.cpp:191] Iteration 80000, loss = 0.779015
I0112 22:26:46.359735 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.760417
I0112 22:26:46.359743 1994072832 solver.cpp:206]     Train net output #1: loss = 0.779015 (* 1 = 0.779015 loss)
I0112 22:26:46.359781 1994072832 solver.cpp:403] Iteration 80000, lr = 0.0019245
I0112 22:27:21.841847 1994072832 solver.cpp:191] Iteration 90000, loss = 0.755328
I0112 22:27:21.841886 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.772917
I0112 22:27:21.841894 1994072832 solver.cpp:206]     Train net output #1: loss = 0.755328 (* 1 = 0.755328 loss)
I0112 22:27:21.841914 1994072832 solver.cpp:403] Iteration 90000, lr = 0.00177828
I0112 22:27:57.207875 1994072832 solver.cpp:317] Snapshotting to .49s8rzwmj_iter_100000.caffemodel
I0112 22:27:57.230962 1994072832 solver.cpp:324] Snapshotting solver state to .49s8rzwmj_iter_100000.solverstate
I0112 22:27:57.238667 1994072832 solver.cpp:247] Iteration 100000, Testing net (#0)
I0112 22:27:57.724531 1994072832 solver.cpp:298]     Test net output #0: accuracy = 0.76125
I0112 22:27:57.724560 1994072832 solver.cpp:298]     Test net output #1: loss = 0.697903 (* 1 = 0.697903 loss)
I0112 22:27:57.726310 1994072832 solver.cpp:191] Iteration 100000, loss = 0.73117
I0112 22:27:57.726321 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.783333
I0112 22:27:57.726327 1994072832 solver.cpp:206]     Train net output #1: loss = 0.73117 (* 1 = 0.73117 loss)
I0112 22:27:57.726332 1994072832 solver.cpp:403] Iteration 100000, lr = 0.0016556
I0112 22:28:33.152693 1994072832 solver.cpp:191] Iteration 110000, loss = 0.709602
I0112 22:28:33.152732 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.79375
I0112 22:28:33.152740 1994072832 solver.cpp:206]     Train net output #1: loss = 0.709602 (* 1 = 0.709602 loss)
I0112 22:28:33.152776 1994072832 solver.cpp:403] Iteration 110000, lr = 0.00155101
I0112 22:29:08.500681 1994072832 solver.cpp:191] Iteration 120000, loss = 0.691827
I0112 22:29:08.500721 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.791667
I0112 22:29:08.500730 1994072832 solver.cpp:206]     Train net output #1: loss = 0.691827 (* 1 = 0.691827 loss)
I0112 22:29:08.500766 1994072832 solver.cpp:403] Iteration 120000, lr = 0.00146064
I0112 22:29:43.901180 1994072832 solver.cpp:191] Iteration 130000, loss = 0.67487
I0112 22:29:43.901221 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.8
I0112 22:29:43.901228 1994072832 solver.cpp:206]     Train net output #1: loss = 0.67487 (* 1 = 0.67487 loss)
I0112 22:29:43.901233 1994072832 solver.cpp:403] Iteration 130000, lr = 0.00138167
I0112 22:30:19.319005 1994072832 solver.cpp:191] Iteration 140000, loss = 0.660499
I0112 22:30:19.319043 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.797917
I0112 22:30:19.319051 1994072832 solver.cpp:206]     Train net output #1: loss = 0.660499 (* 1 = 0.660499 loss)
I0112 22:30:19.319056 1994072832 solver.cpp:403] Iteration 140000, lr = 0.00131199
I0112 22:30:54.658823 1994072832 solver.cpp:191] Iteration 150000, loss = 0.647731
I0112 22:30:54.658862 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.795833
I0112 22:30:54.658870 1994072832 solver.cpp:206]     Train net output #1: loss = 0.647731 (* 1 = 0.647731 loss)
I0112 22:30:54.658875 1994072832 solver.cpp:403] Iteration 150000, lr = 0.00125
I0112 22:31:30.030968 1994072832 solver.cpp:191] Iteration 160000, loss = 0.635908
I0112 22:31:30.031008 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.8
I0112 22:31:30.031015 1994072832 solver.cpp:206]     Train net output #1: loss = 0.635908 (* 1 = 0.635908 loss)
I0112 22:31:30.031051 1994072832 solver.cpp:403] Iteration 160000, lr = 0.00119444
I0112 22:32:05.399258 1994072832 solver.cpp:191] Iteration 170000, loss = 0.627213
I0112 22:32:05.399298 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.802083
I0112 22:32:05.399307 1994072832 solver.cpp:206]     Train net output #1: loss = 0.627213 (* 1 = 0.627213 loss)
I0112 22:32:05.399343 1994072832 solver.cpp:403] Iteration 170000, lr = 0.00114431
I0112 22:32:41.023105 1994072832 solver.cpp:191] Iteration 180000, loss = 0.618847
I0112 22:32:41.023143 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.808333
I0112 22:32:41.023151 1994072832 solver.cpp:206]     Train net output #1: loss = 0.618847 (* 1 = 0.618847 loss)
I0112 22:32:41.023186 1994072832 solver.cpp:403] Iteration 180000, lr = 0.00109884
I0112 22:33:16.544178 1994072832 solver.cpp:191] Iteration 190000, loss = 0.610482
I0112 22:33:16.544231 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.814583
I0112 22:33:16.544240 1994072832 solver.cpp:206]     Train net output #1: loss = 0.610482 (* 1 = 0.610482 loss)
I0112 22:33:16.544252 1994072832 solver.cpp:403] Iteration 190000, lr = 0.00105737
I0112 22:33:52.148075 1994072832 solver.cpp:317] Snapshotting to .49s8rzwmj_iter_200000.caffemodel
I0112 22:33:52.168805 1994072832 solver.cpp:324] Snapshotting solver state to .49s8rzwmj_iter_200000.solverstate
I0112 22:33:52.176705 1994072832 solver.cpp:247] Iteration 200000, Testing net (#0)
I0112 22:33:52.663600 1994072832 solver.cpp:298]     Test net output #0: accuracy = 0.808937
I0112 22:33:52.663630 1994072832 solver.cpp:298]     Test net output #1: loss = 0.570212 (* 1 = 0.570212 loss)
I0112 22:33:52.665398 1994072832 solver.cpp:191] Iteration 200000, loss = 0.604035
I0112 22:33:52.665410 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.810417
I0112 22:33:52.665416 1994072832 solver.cpp:206]     Train net output #1: loss = 0.604035 (* 1 = 0.604035 loss)
I0112 22:33:52.665421 1994072832 solver.cpp:403] Iteration 200000, lr = 0.00101938
I0112 22:34:28.381541 1994072832 solver.cpp:191] Iteration 210000, loss = 0.597968
I0112 22:34:28.381582 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.810417
I0112 22:34:28.381590 1994072832 solver.cpp:206]     Train net output #1: loss = 0.597968 (* 1 = 0.597968 loss)
I0112 22:34:28.381628 1994072832 solver.cpp:403] Iteration 210000, lr = 0.000984426
I0112 22:35:04.830225 1994072832 solver.cpp:191] Iteration 220000, loss = 0.592936
I0112 22:35:04.830266 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.8125
I0112 22:35:04.830274 1994072832 solver.cpp:206]     Train net output #1: loss = 0.592936 (* 1 = 0.592936 loss)
I0112 22:35:04.830286 1994072832 solver.cpp:403] Iteration 220000, lr = 0.000952147
I0112 22:35:40.860607 1994072832 solver.cpp:191] Iteration 230000, loss = 0.588249
I0112 22:35:40.860648 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.8125
I0112 22:35:40.860657 1994072832 solver.cpp:206]     Train net output #1: loss = 0.588249 (* 1 = 0.588249 loss)
I0112 22:35:40.860671 1994072832 solver.cpp:403] Iteration 230000, lr = 0.000922235
I0112 22:36:17.413177 1994072832 solver.cpp:191] Iteration 240000, loss = 0.582708
I0112 22:36:17.413215 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.810417
I0112 22:36:17.413224 1994072832 solver.cpp:206]     Train net output #1: loss = 0.582708 (* 1 = 0.582708 loss)
I0112 22:36:17.413238 1994072832 solver.cpp:403] Iteration 240000, lr = 0.000894427
I0112 22:36:53.980494 1994072832 solver.cpp:191] Iteration 250000, loss = 0.579171
I0112 22:36:53.980530 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.810417
I0112 22:36:53.980538 1994072832 solver.cpp:206]     Train net output #1: loss = 0.579171 (* 1 = 0.579171 loss)
I0112 22:36:53.980543 1994072832 solver.cpp:403] Iteration 250000, lr = 0.0008685
I0112 22:37:29.946212 1994072832 solver.cpp:191] Iteration 260000, loss = 0.576349
I0112 22:37:29.946249 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.810417
I0112 22:37:29.946257 1994072832 solver.cpp:206]     Train net output #1: loss = 0.576349 (* 1 = 0.576349 loss)
I0112 22:37:29.946270 1994072832 solver.cpp:403] Iteration 260000, lr = 0.000844262
I0112 22:38:05.996740 1994072832 solver.cpp:191] Iteration 270000, loss = 0.573126
I0112 22:38:05.996779 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.810417
I0112 22:38:05.996788 1994072832 solver.cpp:206]     Train net output #1: loss = 0.573126 (* 1 = 0.573126 loss)
I0112 22:38:05.996793 1994072832 solver.cpp:403] Iteration 270000, lr = 0.000821545
I0112 22:38:41.835878 1994072832 solver.cpp:191] Iteration 280000, loss = 0.569269
I0112 22:38:41.835917 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.808333
I0112 22:38:41.835925 1994072832 solver.cpp:206]     Train net output #1: loss = 0.569269 (* 1 = 0.569269 loss)
I0112 22:38:41.835930 1994072832 solver.cpp:403] Iteration 280000, lr = 0.000800205
I0112 22:39:17.368101 1994072832 solver.cpp:191] Iteration 290000, loss = 0.56683
I0112 22:39:17.368157 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.808333
I0112 22:39:17.368166 1994072832 solver.cpp:206]     Train net output #1: loss = 0.56683 (* 1 = 0.56683 loss)
I0112 22:39:17.368178 1994072832 solver.cpp:403] Iteration 290000, lr = 0.000780116
I0112 22:39:53.503875 1994072832 solver.cpp:317] Snapshotting to .49s8rzwmj_iter_300000.caffemodel
I0112 22:39:53.525470 1994072832 solver.cpp:324] Snapshotting solver state to .49s8rzwmj_iter_300000.solverstate
I0112 22:39:53.535539 1994072832 solver.cpp:247] Iteration 300000, Testing net (#0)
I0112 22:39:54.040679 1994072832 solver.cpp:298]     Test net output #0: accuracy = 0.821662
I0112 22:39:54.040709 1994072832 solver.cpp:298]     Test net output #1: loss = 0.536608 (* 1 = 0.536608 loss)
I0112 22:39:54.042510 1994072832 solver.cpp:191] Iteration 300000, loss = 0.564326
I0112 22:39:54.042527 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.808333
I0112 22:39:54.042534 1994072832 solver.cpp:206]     Train net output #1: loss = 0.564326 (* 1 = 0.564326 loss)
I0112 22:39:54.042541 1994072832 solver.cpp:403] Iteration 300000, lr = 0.000761165
I0112 22:40:29.824051 1994072832 solver.cpp:191] Iteration 310000, loss = 0.561773
I0112 22:40:29.824091 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.80625
I0112 22:40:29.824100 1994072832 solver.cpp:206]     Train net output #1: loss = 0.561773 (* 1 = 0.561773 loss)
I0112 22:40:29.824105 1994072832 solver.cpp:403] Iteration 310000, lr = 0.000743254
I0112 22:41:06.489987 1994072832 solver.cpp:191] Iteration 320000, loss = 0.559508
I0112 22:41:06.490046 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.80625
I0112 22:41:06.490074 1994072832 solver.cpp:206]     Train net output #1: loss = 0.559508 (* 1 = 0.559508 loss)
I0112 22:41:06.490089 1994072832 solver.cpp:403] Iteration 320000, lr = 0.000726297
I0112 22:41:43.254760 1994072832 solver.cpp:191] Iteration 330000, loss = 0.557995
I0112 22:41:43.254798 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.80625
I0112 22:41:43.254807 1994072832 solver.cpp:206]     Train net output #1: loss = 0.557995 (* 1 = 0.557995 loss)
I0112 22:41:43.254843 1994072832 solver.cpp:403] Iteration 330000, lr = 0.000710217
I0112 22:42:19.153308 1994072832 solver.cpp:191] Iteration 340000, loss = 0.555997
I0112 22:42:19.153347 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.80625
I0112 22:42:19.153354 1994072832 solver.cpp:206]     Train net output #1: loss = 0.555997 (* 1 = 0.555997 loss)
I0112 22:42:19.153362 1994072832 solver.cpp:403] Iteration 340000, lr = 0.000694943
I0112 22:42:54.890751 1994072832 solver.cpp:191] Iteration 350000, loss = 0.554054
I0112 22:42:54.890792 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.80625
I0112 22:42:54.890800 1994072832 solver.cpp:206]     Train net output #1: loss = 0.554054 (* 1 = 0.554054 loss)
I0112 22:42:54.890837 1994072832 solver.cpp:403] Iteration 350000, lr = 0.000680414
I0112 22:43:30.651521 1994072832 solver.cpp:191] Iteration 360000, loss = 0.553137
I0112 22:43:30.651561 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.810417
I0112 22:43:30.651571 1994072832 solver.cpp:206]     Train net output #1: loss = 0.553137 (* 1 = 0.553137 loss)
I0112 22:43:30.651608 1994072832 solver.cpp:403] Iteration 360000, lr = 0.000666575
I0112 22:44:06.572209 1994072832 solver.cpp:191] Iteration 370000, loss = 0.551422
I0112 22:44:06.572247 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.8125
I0112 22:44:06.572257 1994072832 solver.cpp:206]     Train net output #1: loss = 0.551422 (* 1 = 0.551422 loss)
I0112 22:44:06.572260 1994072832 solver.cpp:403] Iteration 370000, lr = 0.000653375
I0112 22:44:43.823003 1994072832 solver.cpp:191] Iteration 380000, loss = 0.549958
I0112 22:44:43.823040 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.8125
I0112 22:44:43.823047 1994072832 solver.cpp:206]     Train net output #1: loss = 0.549958 (* 1 = 0.549958 loss)
I0112 22:44:43.823052 1994072832 solver.cpp:403] Iteration 380000, lr = 0.000640769
I0112 22:45:20.718725 1994072832 solver.cpp:191] Iteration 390000, loss = 0.54921
I0112 22:45:20.718777 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.814583
I0112 22:45:20.718786 1994072832 solver.cpp:206]     Train net output #1: loss = 0.54921 (* 1 = 0.54921 loss)
I0112 22:45:20.718792 1994072832 solver.cpp:403] Iteration 390000, lr = 0.000628717
I0112 22:45:56.759330 1994072832 solver.cpp:317] Snapshotting to .49s8rzwmj_iter_400000.caffemodel
I0112 22:45:56.779969 1994072832 solver.cpp:324] Snapshotting solver state to .49s8rzwmj_iter_400000.solverstate
I0112 22:45:56.789304 1994072832 solver.cpp:247] Iteration 400000, Testing net (#0)
I0112 22:45:57.276026 1994072832 solver.cpp:298]     Test net output #0: accuracy = 0.825587
I0112 22:45:57.276054 1994072832 solver.cpp:298]     Test net output #1: loss = 0.525772 (* 1 = 0.525772 loss)
I0112 22:45:57.277793 1994072832 solver.cpp:191] Iteration 400000, loss = 0.548194
I0112 22:45:57.277806 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.814583
I0112 22:45:57.277812 1994072832 solver.cpp:206]     Train net output #1: loss = 0.548194 (* 1 = 0.548194 loss)
I0112 22:45:57.277817 1994072832 solver.cpp:403] Iteration 400000, lr = 0.00061718
I0112 22:46:33.196667 1994072832 solver.cpp:191] Iteration 410000, loss = 0.54688
I0112 22:46:33.196707 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.816667
I0112 22:46:33.196715 1994072832 solver.cpp:206]     Train net output #1: loss = 0.54688 (* 1 = 0.54688 loss)
I0112 22:46:33.196722 1994072832 solver.cpp:403] Iteration 410000, lr = 0.000606126
I0112 22:47:09.348732 1994072832 solver.cpp:191] Iteration 420000, loss = 0.545391
I0112 22:47:09.348770 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.816667
I0112 22:47:09.348778 1994072832 solver.cpp:206]     Train net output #1: loss = 0.545391 (* 1 = 0.545391 loss)
I0112 22:47:09.348783 1994072832 solver.cpp:403] Iteration 420000, lr = 0.000595523
I0112 22:47:45.094288 1994072832 solver.cpp:191] Iteration 430000, loss = 0.544301
I0112 22:47:45.094326 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.814583
I0112 22:47:45.094334 1994072832 solver.cpp:206]     Train net output #1: loss = 0.544301 (* 1 = 0.544301 loss)
I0112 22:47:45.094339 1994072832 solver.cpp:403] Iteration 430000, lr = 0.000585343
I0112 22:48:21.127946 1994072832 solver.cpp:191] Iteration 440000, loss = 0.543204
I0112 22:48:21.127984 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.816667
I0112 22:48:21.127991 1994072832 solver.cpp:206]     Train net output #1: loss = 0.543204 (* 1 = 0.543204 loss)
I0112 22:48:21.127996 1994072832 solver.cpp:403] Iteration 440000, lr = 0.00057556
I0112 22:48:57.208099 1994072832 solver.cpp:191] Iteration 450000, loss = 0.541534
I0112 22:48:57.208137 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.816667
I0112 22:48:57.208147 1994072832 solver.cpp:206]     Train net output #1: loss = 0.541534 (* 1 = 0.541534 loss)
I0112 22:48:57.208183 1994072832 solver.cpp:403] Iteration 450000, lr = 0.00056615
I0112 22:49:34.648038 1994072832 solver.cpp:191] Iteration 460000, loss = 0.539865
I0112 22:49:34.648077 1994072832 solver.cpp:206]     Train net output #0: accuracy = 0.816667
I0112 22:49:34.648085 1994072832 solver.cpp:206]     Train net output #1: loss = 0.539865 (* 1 = 0.539865 loss)
I0112 22:49:34.648092 1994072832 solver.cpp:403] Iteration 460000, lr = 0.000557092
